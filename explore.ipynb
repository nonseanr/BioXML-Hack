{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm2receptor = pd.read_csv('data/pairs.csv', sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mixture\n",
       "mono              39230\n",
       "sum of isomers    10920\n",
       "mixture            1265\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2receptor['Mixture'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Species\n",
       "homo sapiens              40898\n",
       "mus musculus              10242\n",
       "pan troglodytes              55\n",
       "pongo abelii                 50\n",
       "bos taurus                   41\n",
       "pan paniscus                 26\n",
       "ovis aries                   25\n",
       "canis lupus familiaris       25\n",
       "vulpes vulpes                25\n",
       "puma concolor                25\n",
       "rattus norvegicus             3\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2receptor['Species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Mutation\n",
       "D296N                                                                                                                            143\n",
       "+1M                                                                                                                              124\n",
       "Q293R                                                                                                                            122\n",
       "M81V                                                                                                                              97\n",
       "C149W                                                                                                                             67\n",
       "L40Q                                                                                                                              64\n",
       "V118M_Q234R                                                                                                                       64\n",
       "F75S                                                                                                                              64\n",
       "L48M_C65S_Y74F_G97S_W99C_Y103F                                                                                                    63\n",
       "G143R                                                                                                                             63\n",
       "+1M_+2S_+3L_+4F_+5P_+6Q_+7R_+8N_+9L_+10D_+11A                                                                                     63\n",
       "M51T                                                                                                                              63\n",
       "G15E                                                                                                                              63\n",
       "M1-_L2-_Q3-_N4-_Q5-_D6-_T7-                                                                                                       63\n",
       "M200I                                                                                                                             63\n",
       "S294N                                                                                                                             63\n",
       "S272P                                                                                                                             63\n",
       "V283L                                                                                                                             63\n",
       "P242S                                                                                                                             63\n",
       "+1M_+2Q_+3L_+4V_+5L_+6L_+7L_+8M_+9F_+10L_+11L_+12V_+13F_+14I_+15G_+16N_+17T_+18A_+19P_+20A_+21F_+22S_+23V_+24T_+25L_+26E_+27S     63\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2receptor['Mutation'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sm2receptor['Gene Name'].value_counts(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(sm2receptor['Molecule Name'].value_counts(), bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Responsive\n",
       "0    48307\n",
       "1     3108\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2receptor['Responsive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm2lang = pd.read_csv('data/Multi-Labelled_Smiles_Odors_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sm2lang = sm2lang.rename({'nonStereoSMILES': 'SMILES'}, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "descriptors\n",
       "odorless                                     199\n",
       "fruity                                        42\n",
       "waxy                                          24\n",
       "fatty                                         24\n",
       "sulfurous                                     20\n",
       "woody                                         18\n",
       "sweet                                         16\n",
       "green                                         13\n",
       "caramellic                                    13\n",
       "spicy                                         12\n",
       "fatty;oily                                    11\n",
       "herbal                                        10\n",
       "musk                                           9\n",
       "fatty;waxy                                     9\n",
       "floral                                         9\n",
       "phenolic                                       8\n",
       "gassy                                          8\n",
       "vegetable;cognac;alcoholic;green;ethereal      8\n",
       "fishy                                          8\n",
       "balsamic                                       8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2lang['descriptors'].value_counts().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((51415, 14), (4983, 140))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2receptor.shape, sm2lang.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Responsive\n",
       "0    10213\n",
       "1      485\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2receptor.merge(sm2lang, on='SMILES', how='inner')['Responsive'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from protein_encoder import ProteinEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ricomeinl/miniconda3/envs/biomlhack/lib/python3.10/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prot_enc = ProteinEncoder(config_path='facebook/esm2_t6_8M_UR50D', out_dim=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('mps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "prot_enc.model = prot_enc.model.to(device)\n",
    "prot_enc.out = prot_enc.out.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(589,\n",
       " 1254,\n",
       " UniProt ID     589\n",
       " Sequence      1254\n",
       " dtype: int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sm2receptor['UniProt ID'].nunique(), sm2receptor['Sequence'].nunique(), sm2receptor[['UniProt ID', 'Sequence']].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniprot2seq = sm2receptor[['UniProt ID', 'Sequence']].set_index('UniProt ID')['Sequence'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = prot_enc.tokenizer.encode(uniprot2seq['Q8NGN2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "protein_inputs = prot_enc.tokenizer.batch_encode_plus(uniprot2seq['Q8NGN2'], return_tensors=\"pt\", padding=True)\n",
    "protein_inputs = {k: v.to(device) for k, v in protein_inputs.items()}\n",
    "output, _ = prot_enc.forward(protein_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 60.15 GB, other allocations: 1.44 GB, max allowed: 61.20 GB). Tried to allocate 28.98 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m emb \u001b[38;5;241m=\u001b[39m \u001b[43mprot_enc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_repr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43muniprot2seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/Desktop/projects/biomlhackathon/protein_encoder.py:65\u001b[0m, in \u001b[0;36mProteinEncoder.get_repr\u001b[0;34m(self, proteins, batch_size, verbose)\u001b[0m\n\u001b[1;32m     61\u001b[0m     protein_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mbatch_encode_plus(proteins[i:i \u001b[38;5;241m+\u001b[39m batch_size],\n\u001b[1;32m     62\u001b[0m                                                       return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     63\u001b[0m                                                       padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     64\u001b[0m     protein_inputs \u001b[38;5;241m=\u001b[39m {k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m protein_inputs\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 65\u001b[0m     output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprotein_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     67\u001b[0m     protein_repr\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m     69\u001b[0m protein_repr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(protein_repr, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/projects/biomlhackathon/protein_encoder.py:85\u001b[0m, in \u001b[0;36mProteinEncoder.forward\u001b[0;34m(self, inputs, get_mask_logits)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: \u001b[38;5;28mdict\u001b[39m, get_mask_logits: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m     73\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;03m    Encode protein sequence into protein representation\u001b[39;00m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;124;03m        mask_logits : [batch, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m     last_hidden_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mesm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlast_hidden_state\n\u001b[1;32m     86\u001b[0m     reprs \u001b[38;5;241m=\u001b[39m last_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     87\u001b[0m     reprs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout(reprs)\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:907\u001b[0m, in \u001b[0;36mEsmModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    898\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    900\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    901\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    902\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    905\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    906\u001b[0m )\n\u001b[0;32m--> 907\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    908\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    909\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    910\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    911\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    912\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    913\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    914\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    916\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    917\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    918\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    919\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    920\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:612\u001b[0m, in \u001b[0;36mEsmEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    601\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    602\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    603\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    609\u001b[0m         output_attentions,\n\u001b[1;32m    610\u001b[0m     )\n\u001b[1;32m    611\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 612\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    623\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:502\u001b[0m, in \u001b[0;36mEsmLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    490\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    491\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    492\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m ):\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;66;03m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    501\u001b[0m     self_attn_past_key_value \u001b[38;5;241m=\u001b[39m past_key_value[:\u001b[38;5;241m2\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m past_key_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 502\u001b[0m     self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;66;03m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:436\u001b[0m, in \u001b[0;36mEsmAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    427\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    433\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    434\u001b[0m ):\n\u001b[1;32m    435\u001b[0m     hidden_states_ln \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states)\n\u001b[0;32m--> 436\u001b[0m     self_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states_ln\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    443\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    445\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(self_outputs[\u001b[38;5;241m0\u001b[39m], hidden_states)\n\u001b[1;32m    446\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (attention_output,) \u001b[38;5;241m+\u001b[39m self_outputs[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/biomlhack/lib/python3.10/site-packages/transformers/models/esm/modeling_esm.py:375\u001b[0m, in \u001b[0;36mEsmSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    371\u001b[0m     attention_probs \u001b[38;5;241m=\u001b[39m attention_probs \u001b[38;5;241m*\u001b[39m head_mask\n\u001b[1;32m    373\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention_probs\u001b[38;5;241m.\u001b[39mto(value_layer\u001b[38;5;241m.\u001b[39mdtype), value_layer)\n\u001b[0;32m--> 375\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m \u001b[43mcontext_layer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpermute\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcontiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    376\u001b[0m new_context_layer_shape \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39msize()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m+\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_head_size,)\n\u001b[1;32m    377\u001b[0m context_layer \u001b[38;5;241m=\u001b[39m context_layer\u001b[38;5;241m.\u001b[39mview(new_context_layer_shape)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 60.15 GB, other allocations: 1.44 GB, max allowed: 61.20 GB). Tried to allocate 28.98 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "emb = prot_enc.get_repr(list(uniprot2seq.values())).numpy(force=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "biomlhack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
